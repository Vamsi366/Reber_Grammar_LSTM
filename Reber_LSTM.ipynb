{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, TimeDistributed\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Reber grammar and embedded Reber grammar\n",
    "default_reber_grammar = [\n",
    "    [(\"B\",1)],  #(state 0) =B=> (state 1)\n",
    "    [(\"T\", 2),(\"P\", 3)],  # (state 1) =T=> (state 2) or =P=> (state 3)\n",
    "    [(\"X\", 5), (\"S\", 2)], # (state 2) =X=> (state 5) or =S=> (state 2)\n",
    "    [(\"T\", 3), (\"V\", 4)], # (state 3) =T=> (state 3) or =V=> (state 4)\n",
    "    [(\"V\", 6), (\"P\", 5)], # (state 4) =V=> (state 6) or =P=> (state 5)\n",
    "    [(\"X\",3), (\"S\", 6)],  # (state 5) =X=> (state 3) or =S=> (state 6)\n",
    "    [(\"E\", None)]         # (state 6) =E=> <EOS>\n",
    "]\n",
    "\n",
    "embedded_reber_grammar = [\n",
    "    [(\"B\",1)],  #(state 0) =B=> (state 1)\n",
    "    [(\"T\", 2),(\"P\", 3)],  # (state 1) =T=> (state 2) or =P=> (state 3)\n",
    "    [(default_reber_grammar,4)], # (state 2) =REBER=> (state 4)\n",
    "    [(default_reber_grammar,5)], # (state 3) =REBER=> (state 5)\n",
    "    [(\"P\", 6)], # (state 4) =P=> (state 6)\n",
    "    [(\"T\",6)],  # (state 5) =T=> (state 3)\n",
    "    [(\"E\", None)]         # (state 6) =E=> <EOS>\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reber_sequence(grammar):\n",
    "    state = 0\n",
    "    sequence = []\n",
    "    while state is not None:\n",
    "        transitions = grammar[state]\n",
    "        symbol, state = transitions[np.random.randint(len(transitions))]\n",
    "        if isinstance(symbol, list):  # If the symbol is a sub-grammar, generate a sub-sequence\n",
    "            sequence += generate_reber_sequence(symbol)\n",
    "        else:\n",
    "            sequence.append(symbol)\n",
    "    return ''.join(sequence)\n",
    "\n",
    "def generate_dataset(grammar, n_samples):\n",
    "    return [generate_reber_sequence(grammar) for _ in range(n_samples)]\n",
    "\n",
    "# Generate dataset using embedded Reber grammar\n",
    "n_samples = 10000\n",
    "sequences = generate_dataset(embedded_reber_grammar, n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10000, 44)\n",
      "y shape: (10000, 44, 8)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "sequences = tokenizer.texts_to_sequences(sequences)\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Prepare input and output\n",
    "X = sequences[:, :-1]  # Input sequences (all but last character)\n",
    "y = sequences[:, 1:]   # Target sequences (all but first character)\n",
    "y = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)  # One-hot encode the target\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"X shape: {X.shape}\")  # Should be (n_samples, max_sequence_length-1)\n",
    "print(f\"y shape: {y.shape}\")  # Should be (n_samples, max_sequence_length-1, vocab_size)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the model\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
    "embedding_dim = 128  # Size of the embedding vectors\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length-1))\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "model.add(TimeDistributed(Dense(units=vocab_size, activation='softmax')))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# verbose level 2 displays more info while training\n",
    "H = model.fit(X_train, y_train, epochs=100, verbose=2, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training results\n",
    "def plot_results(H):\n",
    "    results = pd.DataFrame({\"Train Loss\": H.history['loss'], \"Validation Loss\": H.history['val_loss'],\n",
    "                            \"Train Accuracy\": H.history['accuracy'], \"Validation Accuracy\": H.history['val_accuracy']})\n",
    "    fig, ax = plt.subplots(nrows=2, figsize=(16, 9))\n",
    "    results[[\"Train Loss\", \"Validation Loss\"]].plot(ax=ax[0])\n",
    "    results[[\"Train Accuracy\", \"Validation Accuracy\"]].plot(ax=ax[1])\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot the results\n",
    "plot_results(H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate sequences starting with a seed character\n",
    "def generate_sequence_with_seed(model, tokenizer, seed_text, max_sequence_length, num_chars_to_generate):\n",
    "    # Tokenize the seed text\n",
    "    seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    generated_sequence = seed_sequence\n",
    "    \n",
    "    # Generate characters one by one\n",
    "    for _ in range(num_chars_to_generate):\n",
    "        # Pad the current sequence\n",
    "        padded_sequence = pad_sequences([generated_sequence], maxlen=max_sequence_length-1, padding='post')\n",
    "        \n",
    "        # Predict the next character\n",
    "        predictions = model.predict(padded_sequence)\n",
    "        predicted_char_index = np.argmax(predictions[0][-1])\n",
    "        \n",
    "        # Add the predicted character to the sequence\n",
    "        generated_sequence.append(predicted_char_index)\n",
    "    \n",
    "    # Convert the sequence of indices back to characters\n",
    "    predicted_sequence = ''.join([tokenizer.index_word.get(index, '') for index in generated_sequence])\n",
    "    \n",
    "    return predicted_sequence\n",
    "\n",
    "# Example usage\n",
    "seed_text = \"B\"  # Starting with a valid seed character\n",
    "num_chars_to_generate = 20  # Number of characters to generate\n",
    "predicted_sequence = generate_sequence_with_seed(model, tokenizer, seed_text, max_sequence_length, num_chars_to_generate)\n",
    "\n",
    "print(\"Generated sequence: \", predicted_sequence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
